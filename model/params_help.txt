"data_dir": "/media/ecalabr/scratch/qc_complete"
    The full path to the directory contaning the individual study directories to be used for training the model.

"model_dir": "same",
    The full path to the directory for model outputs. If "same" is passed, then the data directory is used for model outputs.

"overwrite": "yes",
    Whether or not to overwrite model ouputs if the model is run again. Must be "yes" or "no". Note that specifying "no" will raise an error if a model output already exists in model_dir.

"restore_dir": "last_weights",
    The prefix for the model weights to be used for restoring a training session. "last_weights" is typical, but "best_weights" could also be use.

"data_prefix": ["T1_wmtb", "T2_wmtb", "FLAIR_wmtb", "DWI_wmtb", "ASL_wm", "SWI_wmtb", "DTI_eddy_MD_wm"],
    The prefixes for the input image files specified as a bracketed list. The expected filename format is directory-name_data-prefix.nii.gz i.e. 01234567_flair.nii.gz.

"label_prefix": ["T1gad_wmtb"],
    The prefixes for the label image files specified as a bracketed list. The expected filename format is directory-name_label-prefix.nii.gz i.e. 01234567_labels.nii.gz.

"mask_prefix": ["combined_brain_mask"],
    The prefixes for the input image mask file specified as a bracketed list. The expected filename format is directory-name_mask-prefix.nii.gz i.e. 01234567_brainmask.nii.gz.

"mask_dilate": [193, 229, 0],
    How much to dilate the input mask after cropping the input images to the tight bounding box of the mask. If an integer, then all dimensions are symmetrically expanded by that amount (1/2 before, 1/2 after). If a list of 3 ints, then each masked input image dimension is expanded to the size of the corresponding int. If the int is 0 or otherwise smaller than the corresponding masked input dim, then no expansion is performed in that dimension.

"dimension_mode": "2.5D",
    The dimensionality of the netowrk. Must be "2D", "2.5D", or "3D". 2D and 3D are relatively self explanatory. 2.5D uses 3D slab for training, but much more heavily weights the center slice, and uses only the center slice for inference.

"data_plane": "ax",
    The data plane for the input images. Must be "ax", "cor", or "sag". If ax, the data is not rotated, if cor or sag the data is rotated assuming it was originally in axial orientation.

"train_dims": [80, 80, 9],
    The dimensions for the train patch. This must be a list of 2 (if using 2D mode) or 3 (if using 2.5D or 3D mode) ints. This value will be passed to extract_image_patches or extract_volume_patches. If patch training is not desired, simply make the patches bigger than the input images and set overlap to 1. For 2.5D mode, set the z dimension here and make sure the overlap for z is the same number, i.e. complete overlap.

"train_patch_overlap": [1, 1, 9],
    A list of 2 (if using 2D mode) or 3 (if using 2.5D or 3D mode) ints that serve as the divisior for determining the stride passed to extract_image_patches or extract_volume_patches. The training dimension is divided by the corresponding overlap value to determine the stride. For stride=1 (complete overlap) pass the same value as the corresponding train dim (this is used for 2.5D training to ensure stride 1 in z dimension).

"infer_dims": [200, 232, 9],
    The dimensions for the infer patch. This must be a list of 2 (if using 2D mode) or 3 (if using 2.5D or 3D mode) ints. This value will be passed to extract_image_patches or extract_volume_patches. If patch inference is not desired, simply make the patches bigger than the input images. Patch inference is only supported for 3D networks. 2D and 2.5D networks should use infer dims that are larger than the inputs.

"infer_patch_overlap": [1, 1, 9],
    A list of 2 (if using 2D mode) or 3 (if using 2.5D or 3D mode) ints that serve as the divisior for determining the stride passed to extract_image_patches or extract_volume_patches. The inference dimension is divided by the corresponding overlap value to determine the stride. For stride=1 (complete overlap) pass the same value as the corresponding infer dim (this is used for 2.5D inference to ensure stride 1 in z dimension).

"augment_train_data": "yes",
    Whether or not to randomly rotate the training data (in the axial plane). Must be "yes" or "no"

"label_interp": 1,
    The order of interpolation to be used when randomly rotating the label data. This only matters if augment_train_data == yes. 0 = nearest neighbor, 1 = linear, 2 = cubic, etc

"model_name": "unet_25D_bneck",
    The name of the network that is going to be trained. This must be defined in net_builder.py

"base_filters": 32,
    The base number of filters used in network construction. Should be divisible by 4 when using bottleneck layers.

"layer_layout": [3, 4, 6],
    The layer layout parameter used during network construction. This is used differently by each network in net_builder.py

"kernel_size": [3, 3, 1],
    The kernel size used during network construction.

"data_format": "channels_last",
    The tensorflow data format. Must be either "channels_last" or "channels_first". Either should work regardless of the model or input data; however, channels first is incompletely tested.

"activation": "leaky_relu",
    The name of the activation function used during network construction. This must be defined in utils.py loss_picker function.

"buffer_size": 1000,
    Currently unused. This was originally the amount of training samples to prefetch, but this was not found to be efficient or useful.

"shuffle_size": 1000,
    The size of the shuffle buffer. This should be much higher than the number of training patches that come from a single image series, or else it won't really be shuffling the data.

"batch_size": 4,
    The number of patches/samples per training batch.

"num_threads": 4,
    The number of process threads to use for data preprocessing and patch generation on the CPU.

"train_fract": 0.9,
    The fraction of input series/directories to be used for training. The rest are used for evaluation.

"learning_rate": [0.001, 20000, 0.25],
    The learning rate for the model. This is used differently by the different learning rate functions defined in utils.py

"learning_rate_decay": "exponential",
    The name of the learning rate decay function to use. This must be specified in utils.py learning_rate_picker.

"loss": "MSE",
    The name of the loss function to use. This must be specified in utils.py loss_picker.

"num_epochs":1,
    The number of training epochs to use. I.e. the number of times to iterate through all of the training data.

"dropout_rate": 0.3,
    The dropout fraction. This is only used if the network speficied in model_name has a dropout function.

"save_summary_steps": 100
    The number of training steps after which a training summary should be saved. This value is passed directly to tensorflow save_summary_steps.